import tensorflow as tf
slim = tf.contrib.slim


def full_rank_bilinear_pooling(conv, num_classes, dropout, is_training = None, reuse= None, scope='full_rank_bilinear_pooling'):
    with tf.variable_scope (scope, reuse=reuse):
        phi_I = tf.einsum('ijkm,ijkn->imn', conv, conv)
        phi_I = tf.reshape(phi_I, [-1, 512 * 512])
        phi_I = tf.divide(phi_I, 784.0)
        y_ssqrt = tf.multiply(tf.sign(phi_I), tf.sqrt(tf.abs(phi_I) + 1e-12))
        z_l2 = tf.nn.l2_normalize(y_ssqrt, dim=1)
        # z_l2 = slim.dropout(z_l2,dropout,is_training=is_training)
        # logits = slim.fully_connected(z_l2,num_classes, activation_fn=None, weights_initializer=tf.zeros_initializer(),
        #                            biases_initializer=tf.zeros_initializer(), trainable=is_training)
        logits = slim.fully_connected(z_l2, num_classes, activation_fn=None, trainable=is_training)
        return logits
def low_rank_bilinear_pooling(conv, num_classes, dropout, is_training = None, reuse= None, scope='low_rank_bilinear_pooling'):
    with tf.variable_scope (scope, reuse=reuse):
        conv_a = slim.conv2d(conv, num_classes, [1,1], scope='project_a', weights_initializer=tf.contrib.layers.xavier_initializer(0.001),
                    biases_initializer=tf.zeros_initializer(),activation_fn=None, normalizer_fn=None)

        conv_b = slim.conv2d(conv, num_classes, [1, 1], scope='project_b',weights_initializer=tf.contrib.layers.xavier_initializer(0.001),
                             biases_initializer=tf.zeros_initializer(), activation_fn=None, normalizer_fn=None)
        phi_I = tf.einsum('ijkm,ijkn->imn', conv_a, conv_b)
        phi_I = tf.reshape(phi_I, [-1, num_classes * num_classes])
        phi_I = tf.divide(phi_I, 784.0)
        y_ssqrt = tf.multiply(tf.sign(phi_I), tf.sqrt(tf.abs(phi_I) + 1e-12))
        z_l2 = tf.nn.l2_normalize(y_ssqrt, dim=1)
        z_l2 = slim.dropout(z_l2, dropout, is_training=is_training)
        logits = slim.fully_connected(z_l2, num_classes, activation_fn=None)
        return logits

def euclidean_norm(tensor, reduction_indicies = None, name = None):
    with tf.op_scope(tensor + reduction_indicies, name, "euclidean_norm"): #need to have this for tf to work
        squareroot_tensor = tf.square(tensor)
        euclidean_norm = tf.sum(squareroot_tensor, reduction_indicies =  reduction_indicies)
        return euclidean_norm

def frobenius_norm(tensor, reduction_indicies = None, name = None):
    #with tf.op_scope(tensor + reduction_indicies, name, "frobenius_norm"): #need to have this for tf to work
        squareroot_tensor = tf.square(tensor)
        tensor_sum = tf.reduce_sum(squareroot_tensor, axis=reduction_indicies, keep_dims=True)
        frobenius_norm = tf.sqrt(tensor_sum)
        return frobenius_norm

def cov_pooling_operation(cnn, batch_size):
    shape_1 = cnn.get_shape().as_list()
    reshape_1 = tf.reshape(cnn, [-1, shape_1[1] * shape_1[2], shape_1[3]])
    reshape_1 = tf.nn.l2_normalize(reshape_1,dim=[1,2])
    # spd = _cal_cov_pooling(reshape_1,batch_size)
    spd = _cal_gaussian_pooling(reshape_1, batch_size)
    return spd


def _cal_cov_pooling(features, batch_size):
    shape_f = features.get_shape().as_list()
    centers_batch = tf.reduce_mean(tf.transpose(features, [0, 2, 1]),2)
    centers_batch = tf.reshape(centers_batch, [-1, 1, shape_f[2]])
    centers_batch = tf.tile(centers_batch, [1, shape_f[1], 1])
    tmp = tf.subtract(features, centers_batch)
    tmp_t = tf.transpose(tmp, [0, 2, 1])
    features_t = 1/tf.cast((shape_f[1]-1),tf.float32)*tf.matmul(tmp_t, tmp)
    trace_t = tf.trace(features_t)
    trace_t = tf.reshape(trace_t, [-1, 1])
    trace_t = tf.tile(trace_t, [1, shape_f[2]])
    trace_t = 0.0001*tf.matrix_diag(trace_t)
    return tf.add(features_t,trace_t)



def _cal_gaussian_pooling(features, batch_size):
    shape_f = features.get_shape().as_list()
    centers_batch = tf.reduce_mean(tf.transpose(features, [0, 2, 1]),2)
    centers_batch = tf.reshape(centers_batch, [batch_size, 1, shape_f[2]])
    centers_batch_tile = tf.tile(centers_batch, [1, shape_f[1], 1])
    tmp = tf.subtract(features, centers_batch_tile)
    tmp_t = tf.transpose(tmp, [0, 2, 1])
    cov = 1/tf.cast((shape_f[1]-1),tf.float32)*tf.matmul(tmp_t, tmp)
    cov = tf.add(cov, tf.matmul(tf.transpose(centers_batch,[0,2,1]), centers_batch))
    col_right = tf.reshape(centers_batch, [batch_size, shape_f[2], 1])
    new_mat = tf.concat([cov,col_right],2)
    row_bottom = tf.concat([centers_batch,tf.ones([batch_size,1,1])],2)
    features_t = tf.concat([new_mat,row_bottom],1)
    shape_f = features_t.get_shape().as_list()
    trace_t = tf.trace(features_t)
    trace_t = tf.reshape(trace_t, [batch_size, 1])
    trace_t = tf.tile(trace_t, [1, shape_f[2]])
    trace_t = 0.0001*tf.matrix_diag(trace_t)
    return tf.add(features_t,trace_t)

# Implementation of Forbenius norm of eigen values
def _cal_for_norm_cov(mat):
    e, v = tf.self_adjoint_eig(mat)
    inner = tf.clip_by_value(e, 1e-6, 1e6)
    inner = tf.sqrt(inner)
    # inner = tf.log(inner)
    inner/=tf.norm(e)
    # s_f = tf.maximum(0.0,s_f)+1e-6
    inner = tf.matrix_diag(inner)
    # s_f = tf.matrix_diag(tf.sqrt(s_f))
    features_t = tf.matmul(tf.matmul(v, inner), tf.transpose(v, [0, 2, 1]))
    #features_t = tf.matmul(tf.matmul(tf.nn.top_k(u,k, sorted=True).values, inner), tf.transpose(tf.nn.top_k(u,k, sorted=True).values, [0,2,1]))
    return features_t

# computes weights for BiMap Layer
def _variable_with_orth_weight_decay(name1, shape, batch_size, n_times):
    s1 = tf.cast(shape[2], tf.int32)
    s2 = tf.cast(shape[2]/n_times, tf.int32)
    w0_init, _ = tf.qr(tf.random_normal([s1, s2], mean=0.0, stddev=1.0 / tf.sqrt(tf.cast(s1, tf.float32))))
    w0 = tf.get_variable(name1, initializer=w0_init)
    tmp1 = tf.reshape(w0, (1, s1, s2))
    tmp2 = tf.reshape(tf.transpose(w0), (1, s2, s1))
    tmp1 = tf.tile(tmp1, [shape[0], 1, 1])
    tmp2 = tf.tile(tmp2, [shape[0], 1, 1])
    # tmp1 = tf.tile(tmp1, [batch_size, 1, 1])
    # tmp2 = tf.tile(tmp2, [batch_size, 1, 1])
    return tmp1,tmp2

